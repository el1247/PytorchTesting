{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df9348a",
   "metadata": {},
   "source": [
    "# Pytorch Text - Better Language modeling\n",
    "Notebook for following along with Pytorch Text interpretation tutorial, starting with nn.transformer and torchtext [Pytorch](https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html)  website tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec967837",
   "metadata": {},
   "source": [
    "### Choices for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec7a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e6fc7e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699447fc",
   "metadata": {},
   "source": [
    "### Libaries and Modules\n",
    "Importing the necessary libaries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d660636a",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "#Import cell\n",
    "import captum\n",
    "import copy\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.models import RobertaClassificationHead\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device = 'cpu' #Cuda having issues on PC, so manual setting to cpu\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d57c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b8a2f",
   "metadata": {},
   "source": [
    "### Importing and preparing data sets\n",
    "Importing and preparing the data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d9d6461",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Gather datasets and prepare them for consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76441c10",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets successfully imported.\n"
     ]
    }
   ],
   "source": [
    "#Importing data sets\n",
    "small_input_batch = [\"Hello world\", \"How are you!\"]\n",
    "big_input_batch = [\"Hello world\", \"How are you!\",\n",
    "                   \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\"]\n",
    "\n",
    "print(\"Data sets successfully imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b051ffe",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders defined, running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Loader definitions\n",
    "\n",
    "print(f\"Loaders defined, running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4b04821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21efa4f7030>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting seed value\n",
    "torch.manual_seed(1247)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f41a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e919b",
   "metadata": {},
   "source": [
    "### Class Definitions\n",
    "<b>Classes:</b><br>\n",
    "<ul>\n",
    "    <li>TransformerModel - Language interpretting model.</li>\n",
    "    <li>PositionalEncoding - Injects information about the relative or absolute position of tokens in the sequence.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f3a52d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes defined.\n"
     ]
    }
   ],
   "source": [
    "#Class definition cell\n",
    "\n",
    "print(\"Classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d966b5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae43c80",
   "metadata": {},
   "source": [
    "### Calculation functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "    <li>get_batch - generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length bptt. For language modelling, the model needs the following words as Target.</li>\n",
    "    <li>generate_square_subsequent_mask - generates an upper triangular matrix of -inf with zeros on the diagonal.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72ba5a08",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Calculation functions cell\n",
    "\n",
    "print(\"Calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fee989",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0674ba3",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b661aee3",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Plotting functions Cell\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b08738",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e79a7",
   "metadata": {},
   "source": [
    "### Main code\n",
    "#### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89c6f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
    "classifier_head = torchtext.models.RobertaClassificationHead(\n",
    "                    num_classes=2, input_dim = 1024)\n",
    "model = xlmr_large.get_model(head=classifier_head)\n",
    "transform = xlmr_large.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e968f",
   "metadata": {},
   "source": [
    "#### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "342e55be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch = big_input_batch #Change between being and small\n",
    "ITERATIONS = 10\n",
    "\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
    "output = model(model_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d897f6",
   "metadata": {},
   "source": [
    "#### Running Iterations\n",
    "Here the model BT fast path is taken by calling `model.eval()` and disabling gradient collection with `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaae80d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow path:\n",
      "==========\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::eq         0.00%      26.000us         0.00%      26.000us      26.000us             1  \n",
      "            aten::embedding         0.00%      70.000us         0.00%     693.000us     693.000us             1  \n",
      "              aten::reshape         0.00%       4.000us         0.00%       8.000us       8.000us             1  \n",
      "       aten::_reshape_alias         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "         aten::index_select         0.00%     604.000us         0.00%     612.000us     612.000us             1  \n",
      "                aten::empty         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "               aten::select         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::select         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::view         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                   aten::ne         0.00%      14.000us         0.00%      14.000us      14.000us             1  \n",
      "                   aten::to         0.00%       3.000us         0.00%      19.000us      19.000us             1  \n",
      "             aten::_to_copy         0.00%       7.000us         0.00%      16.000us      16.000us             1  \n",
      "        aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                aten::copy_         0.00%       6.000us         0.00%       6.000us       6.000us             1  \n",
      "               aten::cumsum         0.00%      12.000us         0.00%      12.000us      12.000us             1  \n",
      "                   aten::to         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                  aten::mul         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                  aten::add         0.00%      11.000us         0.00%      11.000us      11.000us             1  \n",
      "            aten::embedding         0.00%      11.000us         0.00%     620.000us     620.000us             1  \n",
      "              aten::reshape         0.00%       2.000us         0.00%       3.000us       3.000us             1  \n",
      "       aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "         aten::index_select         0.00%     592.000us         0.00%     604.000us     604.000us             1  \n",
      "                aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::select         0.00%       9.000us         0.00%      10.000us      10.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::select         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                 aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                  aten::add         0.00%       1.061ms         0.00%       1.061ms       1.061ms             1  \n",
      "           aten::layer_norm         0.00%       2.000us         0.00%       2.716ms       2.716ms             1  \n",
      "    aten::native_layer_norm         0.00%       2.689ms         0.00%       2.714ms       2.714ms             1  \n",
      "                aten::empty         0.00%      16.000us         0.00%      16.000us      16.000us             1  \n",
      "                aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::view         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                 aten::view         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "              aten::dropout         0.00%      11.000us         0.00%       3.235ms       3.235ms             1  \n",
      "           aten::empty_like         0.00%       3.000us         0.00%      18.000us      18.000us             1  \n",
      "                aten::empty         0.00%      15.000us         0.00%      15.000us      15.000us             1  \n",
      "           aten::bernoulli_         0.00%       1.522ms         0.00%       1.532ms       1.532ms             1  \n",
      "                aten::empty         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                 aten::div_         0.00%     289.000us         0.00%     303.000us     303.000us             1  \n",
      "                   aten::to         0.00%       2.000us         0.00%      14.000us      14.000us             1  \n",
      "             aten::_to_copy         0.00%       5.000us         0.00%      12.000us      12.000us             1  \n",
      "        aten::empty_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::copy_         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                  aten::mul         0.00%       1.371ms         0.00%       1.371ms       1.371ms             1  \n",
      "            aten::unsqueeze         0.00%       8.000us         0.00%      11.000us      11.000us             1  \n",
      "           aten::as_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "              aten::type_as         0.00%       1.000us         0.00%      17.000us      17.000us             1  \n",
      "                   aten::to         0.00%       1.000us         0.00%      16.000us      16.000us             1  \n",
      "             aten::_to_copy         0.00%       6.000us         0.00%      15.000us      15.000us             1  \n",
      "        aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                aten::copy_         0.00%       6.000us         0.00%       6.000us       6.000us             1  \n",
      "                 aten::rsub         0.00%      31.000us         0.00%     111.000us     111.000us             1  \n",
      "                  aten::sub         0.00%      70.000us         0.00%      80.000us      80.000us             1  \n",
      "                   aten::to         0.00%       1.000us         0.00%      10.000us      10.000us             1  \n",
      "             aten::_to_copy         0.00%       4.000us         0.00%       9.000us       9.000us             1  \n",
      "        aten::empty_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::copy_         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                  aten::mul         0.00%       1.189ms         0.00%       1.189ms       1.189ms             1  \n",
      "            aten::transpose         0.00%      11.000us         0.00%      13.000us      13.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::linear         0.00%       7.000us         0.06%      49.905ms      49.905ms             1  \n",
      "                    aten::t         0.00%       7.000us         0.00%      10.000us      10.000us             1  \n",
      "            aten::transpose         0.00%       1.000us         0.00%       3.000us       3.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::matmul         0.00%      16.000us         0.06%      48.125ms      48.125ms             1  \n",
      "              aten::reshape         0.00%       8.000us         0.00%     831.000us     831.000us             1  \n",
      "                aten::clone         0.00%      11.000us         0.00%     801.000us     801.000us             1  \n",
      "           aten::empty_like         0.00%       3.000us         0.00%      22.000us      22.000us             1  \n",
      "                aten::empty         0.00%      19.000us         0.00%      19.000us      19.000us             1  \n",
      "                aten::copy_         0.00%     768.000us         0.00%     768.000us     768.000us             1  \n",
      "         aten::_unsafe_view         0.00%      22.000us         0.00%      22.000us      22.000us             1  \n",
      "                   aten::mm         0.06%      47.266ms         0.06%      47.267ms      47.267ms             1  \n",
      "         aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "         aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "         aten::_unsafe_view         0.00%      11.000us         0.00%      11.000us      11.000us             1  \n",
      "                 aten::add_         0.00%       1.763ms         0.00%       1.763ms       1.763ms             1  \n",
      "                aten::chunk         0.00%       2.000us         0.00%      44.000us      44.000us             1  \n",
      "                aten::split         0.00%      29.000us         0.00%      42.000us      42.000us             1  \n",
      "               aten::narrow         0.00%       3.000us         0.00%       9.000us       9.000us             1  \n",
      "                aten::slice         0.00%       2.000us         0.00%       6.000us       6.000us             1  \n",
      "           aten::as_strided         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "               aten::narrow         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::slice         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::narrow         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::slice         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "           aten::contiguous         0.00%       3.000us         0.00%     918.000us     918.000us             1  \n",
      "                aten::clone         0.00%      11.000us         0.00%     915.000us     915.000us             1  \n",
      "           aten::empty_like         0.00%       3.000us         0.00%      54.000us      54.000us             1  \n",
      "                aten::empty         0.00%      51.000us         0.00%      51.000us      51.000us             1  \n",
      "                aten::copy_         0.00%     850.000us         0.00%     850.000us     850.000us             1  \n",
      "                 aten::view         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "            aten::transpose         0.00%       6.000us         0.00%       8.000us       8.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 77.116s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "    for i in range(ITERATIONS):\n",
    "        output = model(model_input)\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ffb2fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (token_embedding): Embedding(250002, 1024, padding_idx=1)\n",
       "      (layers): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (12): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (13): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (14): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (15): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (16): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (17): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (18): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (19): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (20): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (21): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (22): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (23): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (positional_embedding): PositionalEmbedding(\n",
       "        (embedding): Embedding(514, 1024, padding_idx=1)\n",
       "      )\n",
       "      (embedding_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (activation_fn): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d96b99be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast path:\n",
      "==========\n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::eq         0.00%      40.000us         0.00%      40.000us      40.000us             1  \n",
      "                         aten::embedding         0.00%      11.000us         0.00%     543.000us     543.000us             1  \n",
      "                           aten::reshape         0.00%       3.000us         0.00%       6.000us       6.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                      aten::index_select         0.00%     515.000us         0.00%     524.000us     524.000us             1  \n",
      "                             aten::empty         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::select         0.00%       3.000us         0.00%       5.000us       5.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::select         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                                aten::ne         0.00%      14.000us         0.00%      14.000us      14.000us             1  \n",
      "                                aten::to         0.00%       2.000us         0.00%      19.000us      19.000us             1  \n",
      "                          aten::_to_copy         0.00%       7.000us         0.00%      17.000us      17.000us             1  \n",
      "                     aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                             aten::copy_         0.00%       7.000us         0.00%       7.000us       7.000us             1  \n",
      "                            aten::cumsum         0.00%      12.000us         0.00%      12.000us      12.000us             1  \n",
      "                                aten::to         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                               aten::mul         0.00%      11.000us         0.00%      11.000us      11.000us             1  \n",
      "                               aten::add         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                         aten::embedding         0.00%       7.000us         0.00%     649.000us     649.000us             1  \n",
      "                           aten::reshape         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                      aten::index_select         0.00%     630.000us         0.00%     636.000us     636.000us             1  \n",
      "                             aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::select         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::select         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                               aten::add         0.00%     738.000us         0.00%     738.000us     738.000us             1  \n",
      "                        aten::layer_norm         0.00%       4.000us         0.00%       1.907ms       1.907ms             1  \n",
      "                 aten::native_layer_norm         0.00%       1.882ms         0.00%       1.903ms       1.903ms             1  \n",
      "                             aten::empty         0.00%      15.000us         0.00%      15.000us      15.000us             1  \n",
      "                             aten::empty         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                           aten::dropout         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                         aten::unsqueeze         0.00%       7.000us         0.00%       9.000us       9.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                           aten::type_as         0.00%       3.000us         0.00%      20.000us      20.000us             1  \n",
      "                                aten::to         0.00%       2.000us         0.00%      17.000us      17.000us             1  \n",
      "                          aten::_to_copy         0.00%       5.000us         0.00%      15.000us      15.000us             1  \n",
      "                     aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                             aten::copy_         0.00%       7.000us         0.00%       7.000us       7.000us             1  \n",
      "                              aten::rsub         0.00%       5.000us         0.00%      21.000us      21.000us             1  \n",
      "                               aten::sub         0.00%       9.000us         0.00%      16.000us      16.000us             1  \n",
      "                                aten::to         0.00%       1.000us         0.00%       7.000us       7.000us             1  \n",
      "                          aten::_to_copy         0.00%       1.000us         0.00%       6.000us       6.000us             1  \n",
      "                     aten::empty_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::copy_         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                               aten::mul         0.00%     682.000us         0.00%     682.000us     682.000us             1  \n",
      "    aten::_transformer_encoder_layer_fwd         0.01%       7.739ms         0.43%     284.872ms     284.872ms             1  \n",
      "                                 aten::t         0.00%       2.000us         0.00%       6.000us       6.000us             1  \n",
      "                         aten::transpose         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                           aten::reshape         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                                aten::mm         0.09%      58.347ms         0.09%      58.347ms      58.347ms             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                      aten::_unsafe_view         0.00%       8.000us         0.00%       8.000us       8.000us             1  \n",
      "       aten::_transform_bias_rescale_qkv         0.01%       4.814ms         0.01%       4.858ms       4.858ms             1  \n",
      "                             aten::empty         0.00%      27.000us         0.00%      27.000us      27.000us             1  \n",
      "                              aten::view         0.00%       6.000us         0.00%       6.000us       6.000us             1  \n",
      "                            aten::narrow         0.00%       3.000us         0.00%       7.000us       7.000us             1  \n",
      "                             aten::slice         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::narrow         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::slice         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::narrow         0.00%       0.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::slice         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                        aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                              aten::view         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                         aten::transpose         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                               aten::bmm         0.01%       8.961ms         0.01%       8.961ms       8.961ms             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::expand         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                        aten::contiguous         0.00%       4.000us         0.01%       9.883ms       9.883ms             1  \n",
      "                             aten::clone         0.00%       6.000us         0.01%       9.879ms       9.879ms             1  \n",
      "                        aten::empty_like         0.00%       3.000us         0.00%      22.000us      22.000us             1  \n",
      "                             aten::empty         0.00%      19.000us         0.00%      19.000us      19.000us             1  \n",
      "                             aten::copy_         0.01%       9.851ms         0.01%       9.851ms       9.851ms             1  \n",
      "                   aten::_masked_softmax         0.03%      22.175ms         0.03%      22.202ms      22.202ms             1  \n",
      "                        aten::empty_like         0.00%       4.000us         0.00%      27.000us      27.000us             1  \n",
      "                     aten::empty_strided         0.00%      23.000us         0.00%      23.000us      23.000us             1  \n",
      "                              aten::view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                           aten::reshape         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                               aten::bmm         0.01%       6.523ms         0.01%       6.523ms       6.523ms             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 66.421s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "    with torch.no_grad():\n",
    "        for i in range(ITERATIONS):\n",
    "            output = model(model_input)\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9ab25",
   "metadata": {},
   "source": [
    "#### Run and benchmark\n",
    "Run and benchmark inference on DEVICE with and without BT fastpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "effe455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BT sparsity setting: False\n",
      "BT sparsity setting: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"BT sparsity setting: \"\n",
    "      f\"{model.encoder.transformer.layers.enable_nested_tensor}\")\n",
    "\n",
    "model.encoder.transformer.layers.enable_nested_tensor=False\n",
    "\n",
    "print(f\"BT sparsity setting: \"\n",
    "      f\"{model.encoder.transformer.layers.enable_nested_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bd080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c26b99a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
