{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df9348a",
   "metadata": {},
   "source": [
    "# Pytorch Text - Better Language modeling\n",
    "Notebook for following along with Pytorch Text interpretation tutorial, looking at better transformer (BT) fastpath, using the [Pytorch](https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html)  website tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec967837",
   "metadata": {},
   "source": [
    "### Choices for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec7a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e6fc7e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699447fc",
   "metadata": {},
   "source": [
    "### Libaries and Modules\n",
    "Importing the necessary libaries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d660636a",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "#Import cell\n",
    "import captum\n",
    "import copy\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.models import RobertaClassificationHead\n",
    "from torchtext.functional import to_tensor\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device = 'cpu' #Cuda having issues on PC, so manual setting to cpu\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d57c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b8a2f",
   "metadata": {},
   "source": [
    "### Importing and preparing data sets\n",
    "Importing and preparing the data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9d6461",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Gather datasets and prepare them for consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76441c10",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets successfully imported.\n"
     ]
    }
   ],
   "source": [
    "#Importing data sets\n",
    "small_input_batch = [\"Hello world\", \"How are you!\"]\n",
    "big_input_batch = [\"Hello world\", \"How are you!\",\n",
    "                   \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
    "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
    "if you still try to defend the infamies and horrors perpetrated by\n",
    "that Antichrist- I really believe he is Antichrist- I will have\n",
    "nothing more to do with you and you are no longer my friend, no longer\n",
    "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
    "I have frightened you- sit down and tell me all the news.`\n",
    "\n",
    "It was in July, 1805, and the speaker was the well-known Anna\n",
    "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
    "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
    "of high rank and importance, who was the first to arrive at her\n",
    "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
    "she said, suffering from la grippe; grippe being then a new word in\n",
    "St. Petersburg, used only by the elite.\"\"\"]\n",
    "\n",
    "print(\"Data sets successfully imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b051ffe",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders defined, running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Loader definitions\n",
    "\n",
    "print(f\"Loaders defined, running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b04821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bf93fc0030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting seed value\n",
    "torch.manual_seed(1247)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f41a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e919b",
   "metadata": {},
   "source": [
    "### Class Definitions\n",
    "<b>Classes:</b><br>\n",
    "<ul>\n",
    "    <li>TransformerModel - Language interpretting model.</li>\n",
    "    <li>PositionalEncoding - Injects information about the relative or absolute position of tokens in the sequence.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f3a52d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes defined.\n"
     ]
    }
   ],
   "source": [
    "#Class definition cell\n",
    "\n",
    "print(\"Classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d966b5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae43c80",
   "metadata": {},
   "source": [
    "### Calculation functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ba5a08",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Calculation functions cell\n",
    "\n",
    "print(\"Calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fee989",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0674ba3",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b661aee3",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Plotting functions Cell\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b08738",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e79a7",
   "metadata": {},
   "source": [
    "### Main code\n",
    "#### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c6f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
    "classifier_head = torchtext.models.RobertaClassificationHead(\n",
    "                    num_classes=2, input_dim = 1024)\n",
    "model = xlmr_large.get_model(head=classifier_head)\n",
    "transform = xlmr_large.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e968f",
   "metadata": {},
   "source": [
    "#### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342e55be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch = big_input_batch #Change between being and small\n",
    "ITERATIONS = 10\n",
    "\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
    "output = model(model_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d897f6",
   "metadata": {},
   "source": [
    "#### Running Iterations\n",
    "Here the model BT fast path is taken by calling `model.eval()` and disabling gradient collection with `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaae80d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow path:\n",
      "==========\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::eq         0.00%      28.000us         0.00%      28.000us      28.000us             1  \n",
      "            aten::embedding         0.00%      25.000us         0.00%     604.000us     604.000us             1  \n",
      "              aten::reshape         0.00%       3.000us         0.00%       7.000us       7.000us             1  \n",
      "       aten::_reshape_alias         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "         aten::index_select         0.00%     559.000us         0.00%     569.000us     569.000us             1  \n",
      "                aten::empty         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::select         0.00%       3.000us         0.00%       7.000us       7.000us             1  \n",
      "           aten::as_strided         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "               aten::select         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                 aten::view         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                   aten::ne         0.00%      43.000us         0.00%      43.000us      43.000us             1  \n",
      "                   aten::to         0.00%      10.000us         0.00%      45.000us      45.000us             1  \n",
      "             aten::_to_copy         0.00%      17.000us         0.00%      35.000us      35.000us             1  \n",
      "        aten::empty_strided         0.00%       6.000us         0.00%       6.000us       6.000us             1  \n",
      "                aten::copy_         0.00%      12.000us         0.00%      12.000us      12.000us             1  \n",
      "               aten::cumsum         0.00%      18.000us         0.00%      18.000us      18.000us             1  \n",
      "                   aten::to         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                  aten::mul         0.00%      16.000us         0.00%      16.000us      16.000us             1  \n",
      "                  aten::add         0.00%      14.000us         0.00%      14.000us      14.000us             1  \n",
      "            aten::embedding         0.00%      35.000us         0.00%     547.000us     547.000us             1  \n",
      "              aten::reshape         0.00%       6.000us         0.00%       8.000us       8.000us             1  \n",
      "       aten::_reshape_alias         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "         aten::index_select         0.00%     491.000us         0.00%     501.000us     501.000us             1  \n",
      "                aten::empty         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "               aten::select         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::select         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "           aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                 aten::view         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                  aten::add         0.00%     431.000us         0.00%     431.000us     431.000us             1  \n",
      "           aten::layer_norm         0.00%       5.000us         0.00%     919.000us     919.000us             1  \n",
      "    aten::native_layer_norm         0.00%     894.000us         0.00%     914.000us     914.000us             1  \n",
      "                aten::empty         0.00%      11.000us         0.00%      11.000us      11.000us             1  \n",
      "                aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                aten::empty         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                 aten::view         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                 aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "              aten::dropout         0.00%      24.000us         0.01%       2.697ms       2.697ms             1  \n",
      "           aten::empty_like         0.00%       4.000us         0.00%      25.000us      25.000us             1  \n",
      "                aten::empty         0.00%      21.000us         0.00%      21.000us      21.000us             1  \n",
      "           aten::bernoulli_         0.00%       1.152ms         0.00%       1.161ms       1.161ms             1  \n",
      "                aten::empty         0.00%       9.000us         0.00%       9.000us       9.000us             1  \n",
      "                 aten::div_         0.00%     192.000us         0.00%     210.000us     210.000us             1  \n",
      "                   aten::to         0.00%       3.000us         0.00%      18.000us      18.000us             1  \n",
      "             aten::_to_copy         0.00%       5.000us         0.00%      15.000us      15.000us             1  \n",
      "        aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                aten::copy_         0.00%       7.000us         0.00%       7.000us       7.000us             1  \n",
      "                  aten::mul         0.00%       1.277ms         0.00%       1.277ms       1.277ms             1  \n",
      "            aten::unsqueeze         0.00%       9.000us         0.00%      13.000us      13.000us             1  \n",
      "           aten::as_strided         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "              aten::type_as         0.00%       4.000us         0.00%      27.000us      27.000us             1  \n",
      "                   aten::to         0.00%       2.000us         0.00%      23.000us      23.000us             1  \n",
      "             aten::_to_copy         0.00%       9.000us         0.00%      21.000us      21.000us             1  \n",
      "        aten::empty_strided         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                aten::copy_         0.00%       8.000us         0.00%       8.000us       8.000us             1  \n",
      "                 aten::rsub         0.00%      13.000us         0.00%      32.000us      32.000us             1  \n",
      "                  aten::sub         0.00%      12.000us         0.00%      19.000us      19.000us             1  \n",
      "                   aten::to         0.00%       1.000us         0.00%       7.000us       7.000us             1  \n",
      "             aten::_to_copy         0.00%       1.000us         0.00%       6.000us       6.000us             1  \n",
      "        aten::empty_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::copy_         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                  aten::mul         0.00%     390.000us         0.00%     390.000us     390.000us             1  \n",
      "            aten::transpose         0.00%      10.000us         0.00%      12.000us      12.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::linear         0.00%      20.000us         0.08%      25.431ms      25.431ms             1  \n",
      "                    aten::t         0.00%       6.000us         0.00%       9.000us       9.000us             1  \n",
      "            aten::transpose         0.00%       1.000us         0.00%       3.000us       3.000us             1  \n",
      "           aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "               aten::matmul         0.00%      11.000us         0.08%      25.052ms      25.052ms             1  \n",
      "              aten::reshape         0.00%       6.000us         0.00%     446.000us     446.000us             1  \n",
      "                aten::clone         0.00%       8.000us         0.00%     435.000us     435.000us             1  \n",
      "           aten::empty_like         0.00%       3.000us         0.00%      20.000us      20.000us             1  \n",
      "                aten::empty         0.00%      17.000us         0.00%      17.000us      17.000us             1  \n",
      "                aten::copy_         0.00%     407.000us         0.00%     407.000us     407.000us             1  \n",
      "         aten::_unsafe_view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                   aten::mm         0.08%      24.585ms         0.08%      24.585ms      24.585ms             1  \n",
      "         aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "         aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "         aten::_unsafe_view         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                 aten::add_         0.00%     350.000us         0.00%     350.000us     350.000us             1  \n",
      "                aten::chunk         0.00%       3.000us         0.00%      33.000us      33.000us             1  \n",
      "                aten::split         0.00%      16.000us         0.00%      30.000us      30.000us             1  \n",
      "               aten::narrow         0.00%       5.000us         0.00%      10.000us      10.000us             1  \n",
      "                aten::slice         0.00%       2.000us         0.00%       5.000us       5.000us             1  \n",
      "           aten::as_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "               aten::narrow         0.00%       0.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::slice         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "               aten::narrow         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                aten::slice         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "           aten::contiguous         0.00%       2.000us         0.00%     422.000us     422.000us             1  \n",
      "                aten::clone         0.00%      10.000us         0.00%     420.000us     420.000us             1  \n",
      "           aten::empty_like         0.00%       3.000us         0.00%      27.000us      27.000us             1  \n",
      "                aten::empty         0.00%      24.000us         0.00%      24.000us      24.000us             1  \n",
      "                aten::copy_         0.00%     383.000us         0.00%     383.000us     383.000us             1  \n",
      "                 aten::view         0.00%       9.000us         0.00%       9.000us       9.000us             1  \n",
      "            aten::transpose         0.00%       7.000us         0.00%       8.000us       8.000us             1  \n",
      "           aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 31.696s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"slow path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "    for i in range(ITERATIONS):\n",
    "        output = model(model_input)\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffb2fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (token_embedding): Embedding(250002, 1024, padding_idx=1)\n",
       "      (layers): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (12): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (13): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (14): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (15): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (16): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (17): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (18): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (19): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (20): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (21): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (22): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (23): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (positional_embedding): PositionalEmbedding(\n",
       "        (embedding): Embedding(514, 1024, padding_idx=1)\n",
       "      )\n",
       "      (embedding_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (activation_fn): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d96b99be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast path:\n",
      "==========\n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::eq         0.00%      21.000us         0.00%      21.000us      21.000us             1  \n",
      "                         aten::embedding         0.00%       6.000us         0.00%     480.000us     480.000us             1  \n",
      "                           aten::reshape         0.00%       3.000us         0.00%       5.000us       5.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                      aten::index_select         0.00%     460.000us         0.00%     467.000us     467.000us             1  \n",
      "                             aten::empty         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::select         0.00%       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::select         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::view         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                                aten::ne         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                                aten::to         0.00%       1.000us         0.00%      12.000us      12.000us             1  \n",
      "                          aten::_to_copy         0.00%       5.000us         0.00%      11.000us      11.000us             1  \n",
      "                     aten::empty_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::copy_         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                            aten::cumsum         0.00%      20.000us         0.00%      20.000us      20.000us             1  \n",
      "                                aten::to         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                               aten::mul         0.00%       6.000us         0.00%       6.000us       6.000us             1  \n",
      "                               aten::add         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                         aten::embedding         0.00%       5.000us         0.00%     349.000us     349.000us             1  \n",
      "                           aten::reshape         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                      aten::index_select         0.00%     337.000us         0.00%     341.000us     341.000us             1  \n",
      "                             aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::select         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                        aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                            aten::select         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                               aten::add         0.00%     370.000us         0.00%     370.000us     370.000us             1  \n",
      "                        aten::layer_norm         0.00%       4.000us         0.00%       1.006ms       1.006ms             1  \n",
      "                 aten::native_layer_norm         0.00%     985.000us         0.00%       1.002ms       1.002ms             1  \n",
      "                             aten::empty         0.00%      10.000us         0.00%      10.000us      10.000us             1  \n",
      "                             aten::empty         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::empty         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                           aten::dropout         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                         aten::unsqueeze         0.00%       6.000us         0.00%       7.000us       7.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                           aten::type_as         0.00%       1.000us         0.00%      14.000us      14.000us             1  \n",
      "                                aten::to         0.00%       1.000us         0.00%      13.000us      13.000us             1  \n",
      "                          aten::_to_copy         0.00%       4.000us         0.00%      12.000us      12.000us             1  \n",
      "                     aten::empty_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                             aten::copy_         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                              aten::rsub         0.00%       3.000us         0.00%      14.000us      14.000us             1  \n",
      "                               aten::sub         0.00%       7.000us         0.00%      11.000us      11.000us             1  \n",
      "                                aten::to         0.00%       0.000us         0.00%       4.000us       4.000us             1  \n",
      "                          aten::_to_copy         0.00%       1.000us         0.00%       4.000us       4.000us             1  \n",
      "                     aten::empty_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                             aten::copy_         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                               aten::mul         0.00%     383.000us         0.00%     383.000us     383.000us             1  \n",
      "    aten::_transformer_encoder_layer_fwd         0.60%     166.610ms         1.07%     296.772ms     296.772ms             1  \n",
      "                                 aten::t         0.00%       1.000us         0.00%       8.000us       8.000us             1  \n",
      "                         aten::transpose         0.00%       4.000us         0.00%       7.000us       7.000us             1  \n",
      "                        aten::as_strided         0.00%       3.000us         0.00%       3.000us       3.000us             1  \n",
      "                           aten::reshape         0.00%       3.000us         0.00%       4.000us       4.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                aten::mm         0.05%      13.395ms         0.05%      13.396ms      13.396ms             1  \n",
      "                      aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                      aten::_unsafe_view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "       aten::_transform_bias_rescale_qkv         0.01%       1.700ms         0.01%       1.731ms       1.731ms             1  \n",
      "                             aten::empty         0.00%      19.000us         0.00%      19.000us      19.000us             1  \n",
      "                              aten::view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                            aten::narrow         0.00%       1.000us         0.00%       4.000us       4.000us             1  \n",
      "                             aten::slice         0.00%       1.000us         0.00%       3.000us       3.000us             1  \n",
      "                        aten::as_strided         0.00%       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                            aten::narrow         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                             aten::slice         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                            aten::narrow         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "                             aten::slice         0.00%       0.000us         0.00%       1.000us       1.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                         aten::transpose         0.00%       2.000us         0.00%       3.000us       3.000us             1  \n",
      "                        aten::as_strided         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                               aten::bmm         0.01%       2.339ms         0.01%       2.340ms       2.340ms             1  \n",
      "                      aten::resolve_conj         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::view         0.00%       5.000us         0.00%       5.000us       5.000us             1  \n",
      "                              aten::view         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                            aten::expand         0.00%       3.000us         0.00%       7.000us       7.000us             1  \n",
      "                        aten::as_strided         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                        aten::contiguous         0.00%       1.000us         0.00%     419.000us     419.000us             1  \n",
      "                             aten::clone         0.00%       6.000us         0.00%     418.000us     418.000us             1  \n",
      "                        aten::empty_like         0.00%       2.000us         0.00%      20.000us      20.000us             1  \n",
      "                             aten::empty         0.00%      18.000us         0.00%      18.000us      18.000us             1  \n",
      "                             aten::copy_         0.00%     392.000us         0.00%     392.000us     392.000us             1  \n",
      "                   aten::_masked_softmax         0.07%      20.392ms         0.07%      20.417ms      20.417ms             1  \n",
      "                        aten::empty_like         0.00%       3.000us         0.00%      25.000us      25.000us             1  \n",
      "                     aten::empty_strided         0.00%      22.000us         0.00%      22.000us      22.000us             1  \n",
      "                              aten::view         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n",
      "                              aten::view         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                           aten::reshape         0.00%       1.000us         0.00%       2.000us       2.000us             1  \n",
      "                    aten::_reshape_alias         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                               aten::bmm         0.01%       2.298ms         0.01%       2.298ms       2.298ms             1  \n",
      "                      aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             1  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 27.649s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"fast path:\")\n",
    "print(\"==========\")\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "    with torch.no_grad():\n",
    "        for i in range(ITERATIONS):\n",
    "            output = model(model_input)\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9ab25",
   "metadata": {},
   "source": [
    "#### Run and benchmark\n",
    "Run and benchmark inference on DEVICE with and without BT fastpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effe455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BT sparsity setting: False\n",
      "BT sparsity setting: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"BT sparsity setting: \"\n",
    "      f\"{model.encoder.transformer.layers.enable_nested_tensor}\")\n",
    "\n",
    "model.encoder.transformer.layers.enable_nested_tensor=False\n",
    "\n",
    "print(f\"BT sparsity setting: \"\n",
    "      f\"{model.encoder.transformer.layers.enable_nested_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d1087",
   "metadata": {},
   "source": [
    "The model can also be sped up by running it on the GPU, using if `model.to(DEVICE)` and `model_input = model_input.to(DEVICE)` if `torch.cuda.is_available == True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26b99a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
