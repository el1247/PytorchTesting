{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df9348a",
   "metadata": {},
   "source": [
    "# Pytorch Text - Text Classification with the torchtext library\n",
    "Notebook for following along with Pytorch Text NLP tutorials that is looking to use the torchtext library to build the dataset for text classification analysis [Pytorch](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)  website tutorial. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec967837",
   "metadata": {},
   "source": [
    "### Choices for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d0769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e6fc7e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699447fc",
   "metadata": {},
   "source": [
    "### Libaries and Modules\n",
    "Importing the necessary libaries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d660636a",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu. Cuda available: False\n",
      "Torch current seed = 752224197434400\n",
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "#Import cell\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from io import open\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(1247) #setting seed value\n",
    "print(f\"Device: {device}. Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch current seed = {torch.seed()}\")\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d57c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2827ad3",
   "metadata": {},
   "source": [
    "### Data Loading and Manipulation Functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "    <li>collate_batch - uses pipelines to process input batch of data</li>\n",
    "    <li>yield_tokens - processes data_iter for build_vocab_from_iterator()</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a02c65e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and manipulation functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Data loading and manipulation function definition cell\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "        \n",
    "label_pipeline = lambda x: int(x) - 1        \n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "print(\"Data loading and manipulation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b8a2f",
   "metadata": {},
   "source": [
    "### Importing and preparing data sets\n",
    "Importing and preparing the data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76441c10",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\") \n",
      "\n",
      "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.') \n",
      "\n",
      "(3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\") \n",
      "\n",
      "\n",
      "Data sets successfully imported, running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Importing data sets\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "#Printing demonstration training data\n",
    "for i in range(3): print(next(train_iter), \"\\n\")\n",
    "\n",
    "print(f\"\\nData sets successfully imported, running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d782cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475, 21, 30, 5297]\n",
      "[475, 21, 30, 5297]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Build a vocab with the raw training dataset, generating data batch and iter\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "     \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False,\n",
    "                        collate_fn=collate_batch)\n",
    "\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(text_pipeline('here is an example'))\n",
    "print(label_pipeline('10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a2a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f41a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e919b",
   "metadata": {},
   "source": [
    "### Class Definitions\n",
    "<b>Classes:</b><br>\n",
    "<ul>\n",
    "    <li>TextClassificationModel - nn.Module class with an embedding bag and a linear layer for manipulating torchtext library</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f3a52d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes defined.\n"
     ]
    }
   ],
   "source": [
    "#Class definition cell\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class) -> None:\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        return None\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        return None\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "    \n",
    "print(\"Classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d966b5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae43c80",
   "metadata": {},
   "source": [
    "### Calculation functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ba5a08",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Calculation functions cell\n",
    "    \n",
    "print(\"Calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fee989",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0674ba3",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b661aee3",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Plotting functions Cell\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b08738",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3ee2c",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "    <li>evaluate - evaluation loop, takes dataloader as input, returns accuracy.</li>\n",
    "    <li>train - training loop, takes dataloader as input, no return.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a68dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Training Functions\n",
    "def evaluate(dataloader) -> float:\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1)==label).sum().item()\n",
    "            total_count += label.size(0)       \n",
    "    return total_acc/total_count\n",
    "\n",
    "\n",
    "def train(dataloader) -> None:\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1)==label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx%log_interval == 0 and idx>0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                           total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return None\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e79a7",
   "metadata": {},
   "source": [
    "### Main code\n",
    "The `AG_NEWS` dataset has 4 labels, and therefore for classes:\n",
    "`1: World`, `2: Sports`, `3: Business` and `4:Sci/Tec`. This is defined in [Importing and preparing data sets](#Importing-and-preparing-data-sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ac1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class)\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 5\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23961cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| epoch   1 |   500/ 1782 batches | accuracy    0.687\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.852\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 16.20s | valid accuracy    0.893\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.897\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.902\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.899\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 16.31s | valid accuracy    0.893\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.914\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 16.23s | valid accuracy    0.903\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.925\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.921\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.920\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 15.77s | valid accuracy    0.897\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.937\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.936\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.937\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 15.98s | valid accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.938\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.936\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 15.82s | valid accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.939\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.940\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.939\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 16.40s | valid accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.939\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.940\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 15.98s | valid accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.940\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.940\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 15.84s | valid accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.939\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.938\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.941\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 15.93s | valid accuracy    0.910\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Epoch training loop\n",
    "print('-'*59)\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    \n",
    "    print('-'*59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "         'valid accuracy {:8.3f}'.format(epoch,\n",
    "                                        time.time()-epoch_start_time,\n",
    "                                        accu_val))\n",
    "    print('-'*59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26b99a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
