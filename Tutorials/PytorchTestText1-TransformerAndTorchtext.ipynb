{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df9348a",
   "metadata": {},
   "source": [
    "# Pytorch Text - Language modeling\n",
    "Notebook for following along with Pytorch Text interpretation tutorial, starting with nn.transformer and torchtext [Pytorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) website tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec967837",
   "metadata": {},
   "source": [
    "### Choices for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec7a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e6fc7e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699447fc",
   "metadata": {},
   "source": [
    "### Libaries and Modules\n",
    "Importing the necessary libaries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d660636a",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "#Import cell\n",
    "import captum\n",
    "import copy\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device = 'cpu' #Cuda having issues on PC, so manual setting to cpu\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d57c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b8a2f",
   "metadata": {},
   "source": [
    "### Importing and preparing data sets\n",
    "Importing and preparing the data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9d6461",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Gather datasets and prepare them for consumption\n",
    "train_iter = WikiText2(split='train') #\"consumed to make vocab\"\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensors.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t:t.numel() > 0, data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76441c10",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sets successfully imported.\n"
     ]
    }
   ],
   "source": [
    "#Importing data sets\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "print(\"Data sets successfully imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b051ffe",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders defined, running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Loader definitions\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz seperate sequences, removing extra elements that wouldn't\n",
    "        cleanly fit.   \n",
    "    Args: \n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shap [N // bsz, bsz]\"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, eval_batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "print(f\"Loaders defined, running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b04821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c5bf0bd030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting seed value\n",
    "torch.manual_seed(1247)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f41a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e919b",
   "metadata": {},
   "source": [
    "### Class Definitions\n",
    "<b>Classes:</b><br>\n",
    "<ul>\n",
    "    <li>TransformerModel - Language interpretting model.</li>\n",
    "    <li>PositionalEncoding - Injects information about the relative or absolute position of tokens in the sequence.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f3a52d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes defined.\n"
     ]
    }
   ],
   "source": [
    "#Class definition cell\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                nlayers: int, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "        return None\n",
    "    \n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        return None\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Args: \n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "        Returns:\n",
    "            output Tensor, shape [seq_len, btch_size, ntoken]\"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask) #Returning Nans\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Args: \n",
    "            x: Tensors, shape[seq_len, batch_size, embedding_dim]\"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "print(\"Classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d966b5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae43c80",
   "metadata": {},
   "source": [
    "### Calculation functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "    <li>get_batch - generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length bptt. For language modelling, the model needs the following words as Target.</li>\n",
    "    <li>generate_square_subsequent_mask - generates an upper triangular matrix of -inf with zeros on the diagonal.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ba5a08",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Calculation functions cell\n",
    "bptt = 35\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq)len, batch)size]\n",
    "            and target has shape [seq_len * batch_size]\"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "print(\"Calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fee989",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0674ba3",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b661aee3",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Plotting functions Cell\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b08738",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e79a7",
   "metadata": {},
   "source": [
    "### Main code\n",
    "#### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e539b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "d_hid = 200\n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.2\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid,\n",
    "    nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c2ad3",
   "metadata": {},
   "source": [
    "#### Running the model\n",
    "Here CrossEntropyLoss with a stochastic gradient descent optimizer will be used, with an initial learning rate of 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189999b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e9c317",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    num_batches = len(train_data)//bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt: #only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch%log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time)*1000/log_interval\n",
    "            cur_loss = total_loss/log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | ' \n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | ' \n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return None\n",
    "    \n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11766ca3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5857 batches | lr 5.00 | ms/batch 191.97 | loss  8.02 | ppl  3049.45\n",
      "| epoch   1 |   400/ 5857 batches | lr 5.00 | ms/batch 191.09 | loss  6.85 | ppl   945.58\n",
      "| epoch   1 |   600/ 5857 batches | lr 5.00 | ms/batch 189.79 | loss  6.44 | ppl   627.67\n",
      "| epoch   1 |   800/ 5857 batches | lr 5.00 | ms/batch 190.29 | loss  6.39 | ppl   596.11\n",
      "| epoch   1 |  1000/ 5857 batches | lr 5.00 | ms/batch 191.44 | loss  6.29 | ppl   540.85\n",
      "| epoch   1 |  1200/ 5857 batches | lr 5.00 | ms/batch 193.30 | loss  6.26 | ppl   522.61\n",
      "| epoch   1 |  1400/ 5857 batches | lr 5.00 | ms/batch 195.21 | loss  6.22 | ppl   505.20\n",
      "| epoch   1 |  1600/ 5857 batches | lr 5.00 | ms/batch 198.61 | loss  6.21 | ppl   499.94\n",
      "| epoch   1 |  1800/ 5857 batches | lr 5.00 | ms/batch 198.05 | loss  6.18 | ppl   483.67\n",
      "| epoch   1 |  2000/ 5857 batches | lr 5.00 | ms/batch 200.23 | loss  6.19 | ppl   489.71\n",
      "| epoch   1 |  2200/ 5857 batches | lr 5.00 | ms/batch 199.23 | loss  6.08 | ppl   435.96\n",
      "| epoch   1 |  2400/ 5857 batches | lr 5.00 | ms/batch 198.91 | loss  6.10 | ppl   446.41\n",
      "| epoch   1 |  2600/ 5857 batches | lr 5.00 | ms/batch 196.87 | loss  6.09 | ppl   443.36\n",
      "| epoch   1 |  2800/ 5857 batches | lr 5.00 | ms/batch 196.39 | loss  6.09 | ppl   441.36\n",
      "| epoch   1 |  3000/ 5857 batches | lr 5.00 | ms/batch 194.06 | loss  6.04 | ppl   421.47\n",
      "| epoch   1 |  3200/ 5857 batches | lr 5.00 | ms/batch 198.88 | loss  6.09 | ppl   441.17\n",
      "| epoch   1 |  3400/ 5857 batches | lr 5.00 | ms/batch 197.70 | loss  6.11 | ppl   451.10\n",
      "| epoch   1 |  3600/ 5857 batches | lr 5.00 | ms/batch 202.14 | loss  6.06 | ppl   430.51\n",
      "| epoch   1 |  3800/ 5857 batches | lr 5.00 | ms/batch 201.42 | loss  6.01 | ppl   405.93\n",
      "| epoch   1 |  4000/ 5857 batches | lr 5.00 | ms/batch 201.17 | loss  6.01 | ppl   408.84\n",
      "| epoch   1 |  4200/ 5857 batches | lr 5.00 | ms/batch 198.22 | loss  6.08 | ppl   436.28\n",
      "| epoch   1 |  4400/ 5857 batches | lr 5.00 | ms/batch 196.34 | loss  6.09 | ppl   440.78\n",
      "| epoch   1 |  4600/ 5857 batches | lr 5.00 | ms/batch 200.98 | loss  6.06 | ppl   430.46\n",
      "| epoch   1 |  4800/ 5857 batches | lr 5.00 | ms/batch 198.79 | loss  6.04 | ppl   421.40\n",
      "| epoch   1 |  5000/ 5857 batches | lr 5.00 | ms/batch 199.79 | loss  5.95 | ppl   384.44\n",
      "| epoch   1 |  5200/ 5857 batches | lr 5.00 | ms/batch 198.35 | loss  5.98 | ppl   393.59\n",
      "| epoch   1 |  5400/ 5857 batches | lr 5.00 | ms/batch 197.05 | loss  6.05 | ppl   423.88\n",
      "| epoch   1 |  5600/ 5857 batches | lr 5.00 | ms/batch 194.87 | loss  6.03 | ppl   416.43\n",
      "| epoch   1 |  5800/ 5857 batches | lr 5.00 | ms/batch 195.35 | loss  5.94 | ppl   381.80\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1208.73 | valid loss  6.06 | valid ppl   426.38\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5857 batches | lr 4.75 | ms/batch 201.32 | loss  5.99 | ppl   400.89\n",
      "| epoch   2 |   400/ 5857 batches | lr 4.75 | ms/batch 200.92 | loss  5.94 | ppl   378.14\n",
      "| epoch   2 |   600/ 5857 batches | lr 4.75 | ms/batch 202.11 | loss  5.79 | ppl   325.87\n",
      "| epoch   2 |   800/ 5857 batches | lr 4.75 | ms/batch 198.54 | loss  5.89 | ppl   363.13\n",
      "| epoch   2 |  1000/ 5857 batches | lr 4.75 | ms/batch 198.54 | loss  5.86 | ppl   352.04\n",
      "| epoch   2 |  1200/ 5857 batches | lr 4.75 | ms/batch 196.45 | loss  5.87 | ppl   355.51\n",
      "| epoch   2 |  1400/ 5857 batches | lr 4.75 | ms/batch 196.30 | loss  5.88 | ppl   358.49\n",
      "| epoch   2 |  1600/ 5857 batches | lr 4.75 | ms/batch 196.94 | loss  5.92 | ppl   370.57\n",
      "| epoch   2 |  1800/ 5857 batches | lr 4.75 | ms/batch 196.23 | loss  5.87 | ppl   355.87\n",
      "| epoch   2 |  2000/ 5857 batches | lr 4.75 | ms/batch 194.75 | loss  5.98 | ppl   397.11\n",
      "| epoch   2 |  2200/ 5857 batches | lr 4.75 | ms/batch 201.32 | loss  5.84 | ppl   343.02\n",
      "| epoch   2 |  2400/ 5857 batches | lr 4.75 | ms/batch 196.80 | loss  5.84 | ppl   345.26\n",
      "| epoch   2 |  2600/ 5857 batches | lr 4.75 | ms/batch 198.31 | loss  5.83 | ppl   340.48\n",
      "| epoch   2 |  2800/ 5857 batches | lr 4.75 | ms/batch 198.25 | loss  5.84 | ppl   343.31\n",
      "| epoch   2 |  3000/ 5857 batches | lr 4.75 | ms/batch 199.73 | loss  5.79 | ppl   325.54\n",
      "| epoch   2 |  3200/ 5857 batches | lr 4.75 | ms/batch 196.79 | loss  5.88 | ppl   358.56\n",
      "| epoch   2 |  3400/ 5857 batches | lr 4.75 | ms/batch 190.87 | loss  5.90 | ppl   366.73\n",
      "| epoch   2 |  3600/ 5857 batches | lr 4.75 | ms/batch 190.10 | loss  5.79 | ppl   325.78\n",
      "| epoch   2 |  3800/ 5857 batches | lr 4.75 | ms/batch 190.00 | loss  5.80 | ppl   328.76\n",
      "| epoch   2 |  4000/ 5857 batches | lr 4.75 | ms/batch 190.69 | loss  5.91 | ppl   369.41\n",
      "| epoch   2 |  4200/ 5857 batches | lr 4.75 | ms/batch 189.44 | loss  5.86 | ppl   352.24\n",
      "| epoch   2 |  4400/ 5857 batches | lr 4.75 | ms/batch 187.70 | loss  5.87 | ppl   355.69\n",
      "| epoch   2 |  4600/ 5857 batches | lr 4.75 | ms/batch 190.24 | loss  5.82 | ppl   336.86\n",
      "| epoch   2 |  4800/ 5857 batches | lr 4.75 | ms/batch 188.94 | loss  5.83 | ppl   339.79\n",
      "| epoch   2 |  5000/ 5857 batches | lr 4.75 | ms/batch 188.96 | loss  5.72 | ppl   306.00\n",
      "| epoch   2 |  5200/ 5857 batches | lr 4.75 | ms/batch 188.86 | loss  5.77 | ppl   319.52\n",
      "| epoch   2 |  5400/ 5857 batches | lr 4.75 | ms/batch 188.47 | loss  5.81 | ppl   332.05\n",
      "| epoch   2 |  5600/ 5857 batches | lr 4.75 | ms/batch 188.46 | loss  5.85 | ppl   346.13\n",
      "| epoch   2 |  5800/ 5857 batches | lr 4.75 | ms/batch 188.32 | loss  5.74 | ppl   310.13\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1190.35 | valid loss  5.88 | valid ppl   358.20\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5857 batches | lr 4.51 | ms/batch 192.36 | loss  5.82 | ppl   336.07\n",
      "| epoch   3 |   400/ 5857 batches | lr 4.51 | ms/batch 191.90 | loss  5.77 | ppl   320.70\n",
      "| epoch   3 |   600/ 5857 batches | lr 4.51 | ms/batch 191.30 | loss  5.61 | ppl   273.39\n",
      "| epoch   3 |   800/ 5857 batches | lr 4.51 | ms/batch 191.88 | loss  5.72 | ppl   304.06\n",
      "| epoch   3 |  1000/ 5857 batches | lr 4.51 | ms/batch 191.96 | loss  5.70 | ppl   297.53\n",
      "| epoch   3 |  1200/ 5857 batches | lr 4.51 | ms/batch 191.90 | loss  5.73 | ppl   307.57\n",
      "| epoch   3 |  1400/ 5857 batches | lr 4.51 | ms/batch 194.08 | loss  5.72 | ppl   306.27\n",
      "| epoch   3 |  1600/ 5857 batches | lr 4.51 | ms/batch 192.25 | loss  5.74 | ppl   311.79\n",
      "| epoch   3 |  1800/ 5857 batches | lr 4.51 | ms/batch 192.98 | loss  5.71 | ppl   302.81\n",
      "| epoch   3 |  2000/ 5857 batches | lr 4.51 | ms/batch 193.06 | loss  5.78 | ppl   323.08\n",
      "| epoch   3 |  2200/ 5857 batches | lr 4.51 | ms/batch 194.19 | loss  5.67 | ppl   290.02\n",
      "| epoch   3 |  2400/ 5857 batches | lr 4.51 | ms/batch 191.94 | loss  5.68 | ppl   292.52\n",
      "| epoch   3 |  2600/ 5857 batches | lr 4.51 | ms/batch 191.70 | loss  5.67 | ppl   288.78\n",
      "| epoch   3 |  2800/ 5857 batches | lr 4.51 | ms/batch 192.74 | loss  5.67 | ppl   290.07\n",
      "| epoch   3 |  3000/ 5857 batches | lr 4.51 | ms/batch 193.29 | loss  5.63 | ppl   279.77\n",
      "| epoch   3 |  3200/ 5857 batches | lr 4.51 | ms/batch 192.20 | loss  5.73 | ppl   306.74\n",
      "| epoch   3 |  3400/ 5857 batches | lr 4.51 | ms/batch 191.83 | loss  5.74 | ppl   310.04\n",
      "| epoch   3 |  3600/ 5857 batches | lr 4.51 | ms/batch 192.79 | loss  5.62 | ppl   274.60\n",
      "| epoch   3 |  3800/ 5857 batches | lr 4.51 | ms/batch 192.32 | loss  5.62 | ppl   276.63\n",
      "| epoch   3 |  4000/ 5857 batches | lr 4.51 | ms/batch 192.58 | loss  5.63 | ppl   279.78\n",
      "| epoch   3 |  4200/ 5857 batches | lr 4.51 | ms/batch 196.29 | loss  5.71 | ppl   301.44\n",
      "| epoch   3 |  4400/ 5857 batches | lr 4.51 | ms/batch 192.87 | loss  5.71 | ppl   301.90\n",
      "| epoch   3 |  4600/ 5857 batches | lr 4.51 | ms/batch 194.52 | loss  5.65 | ppl   285.54\n",
      "| epoch   3 |  4800/ 5857 batches | lr 4.51 | ms/batch 193.56 | loss  5.69 | ppl   294.45\n",
      "| epoch   3 |  5000/ 5857 batches | lr 4.51 | ms/batch 193.48 | loss  5.57 | ppl   263.67\n",
      "| epoch   3 |  5200/ 5857 batches | lr 4.51 | ms/batch 194.66 | loss  5.62 | ppl   275.19\n",
      "| epoch   3 |  5400/ 5857 batches | lr 4.51 | ms/batch 192.76 | loss  5.67 | ppl   289.24\n",
      "| epoch   3 |  5600/ 5857 batches | lr 4.51 | ms/batch 194.15 | loss  5.68 | ppl   294.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  5800/ 5857 batches | lr 4.51 | ms/batch 194.18 | loss  5.58 | ppl   264.46\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1184.68 | valid loss  5.92 | valid ppl   373.08\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-'*90)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f} | '\n",
    "         f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-'*90)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b7eac",
   "metadata": {},
   "source": [
    "#### Evaluating the best model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c2c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "| End of training | test loss  5.81 | test ppl   332.42\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print(\"=\"*90)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "     f'test ppl {test_ppl:8.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26b99a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
